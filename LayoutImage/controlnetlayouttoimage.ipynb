{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes diffusers accelerate transformers pycocotools\n!pip install -U peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_script.py\nimport os\nimport random\nimport torch\nfrom pathlib import Path\nfrom PIL import Image, ImageDraw\nfrom tqdm.auto import tqdm\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional\n\nfrom pycocotools.coco import COCO\nfrom torchvision import transforms\nfrom transformers import AutoTokenizer, CLIPTextModel\nfrom diffusers import ControlNetModel, AutoencoderKL, UNet2DConditionModel, DDPMScheduler\nfrom accelerate import Accelerator\nimport bitsandbytes as bnb \n\n@dataclass\nclass TrainingConfig:\n    coco_root: str = \"/kaggle/input/coco-2017-dataset/coco2017\"\n    train_img_dir: str = \"train2017\"\n    train_ann_file: str = \"annotations/instances_train2017.json\"\n    output_dir: str = \"/kaggle/working/controlnet-layout-model\"\n    model_id: str = \"runwayml/stable-diffusion-v1-5\"\n    resolution: int = 512\n    batch_size: int = 2          \n    grad_accumulation: int = 4   \n    num_epochs: int = 10\n    learning_rate: float = 1e-5 \n    max_samples: Optional[int] = 5\n\ndef render_layout_mask(img_size, annotations):\n    canvas = Image.new(\"RGB\", img_size, (0, 0, 0))\n    draw = ImageDraw.Draw(canvas)\n    for ann in annotations:\n        x, y, w, h = ann['bbox']\n        draw.rectangle([x, y, x + w, y + h], outline=(255, 255, 255), width=2)\n    return canvas\n\nclass LayoutConditionedDataset(torch.utils.data.Dataset):\n    def __init__(self, config, tokenizer):\n        self.config = config\n        self.tokenizer = tokenizer\n        self.root_dir = Path(config.coco_root)\n        self.img_dir = self.root_dir / config.train_img_dir\n        self.ann_file = self.root_dir / config.train_ann_file\n        self.coco = COCO(self.ann_file)\n        self.ids = list(self.coco.imgs.keys())\n        self.ids = [i for i in self.ids if len(self.coco.getAnnIds(imgIds=i)) > 0]\n        if config.max_samples: self.ids = self.ids[:config.max_samples]\n\n        self.img_transforms = transforms.Compose([\n            transforms.Resize(config.resolution), transforms.CenterCrop(config.resolution),\n            transforms.ToTensor(), transforms.Normalize([0.5], [0.5]),\n        ])\n        self.mask_transforms = transforms.Compose([\n            transforms.Resize(config.resolution), transforms.CenterCrop(config.resolution),\n            transforms.ToTensor(),\n        ])\n\n    def __len__(self): return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n        img_info = self.coco.loadImgs(img_id)[0]\n        original_img = Image.open(self.img_dir / img_info['file_name']).convert(\"RGB\")\n        anns = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n        control_mask = render_layout_mask(original_img.size, anns)\n        \n        prompt = \"A photorealistic image\" \n        tokens = self.tokenizer(prompt, max_length=77, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0]\n        \n        return {\n            \"pixel_values\": self.img_transforms(original_img),\n            \"conditioning_pixel_values\": self.mask_transforms(control_mask),\n            \"input_ids\": tokens\n        }\n\ndef main():\n    config = TrainingConfig()\n    accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=config.grad_accumulation)\n    \n    if accelerator.is_main_process: print(\"üöÄ Starting Training Script...\")\n\n    tokenizer = AutoTokenizer.from_pretrained(config.model_id, subfolder=\"tokenizer\")\n    noise_scheduler = DDPMScheduler.from_pretrained(config.model_id, subfolder=\"scheduler\")\n    vae = AutoencoderKL.from_pretrained(config.model_id, subfolder=\"vae\")\n    text_encoder = CLIPTextModel.from_pretrained(config.model_id, subfolder=\"text_encoder\")\n    unet = UNet2DConditionModel.from_pretrained(config.model_id, subfolder=\"unet\")\n\n    vae.enable_slicing()\n    vae.requires_grad_(False)\n    text_encoder.requires_grad_(False)\n    unet.requires_grad_(False)\n\n    controlnet = ControlNetModel.from_unet(unet)\n    controlnet.train()\n    controlnet.enable_gradient_checkpointing()\n\n    optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=config.learning_rate)\n    dataset = LayoutConditionedDataset(config, tokenizer)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n\n    controlnet, optimizer, dataloader = accelerator.prepare(controlnet, optimizer, dataloader)\n    vae.to(accelerator.device)\n    text_encoder.to(accelerator.device)\n    unet.to(accelerator.device)\n\n    for epoch in range(config.num_epochs):\n        if accelerator.is_main_process: pbar = tqdm(total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n        \n        for batch in dataloader:\n            with accelerator.accumulate(controlnet):\n                latents = vae.encode(batch[\"pixel_values\"].to(torch.float32)).latent_dist.sample() * 0.18215\n                noise = torch.randn_like(latents)\n                bsz = latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n                \n                enc_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n                \n                down, mid = controlnet(noisy_latents, timesteps, encoder_hidden_states=enc_hidden_states, controlnet_cond=batch[\"conditioning_pixel_values\"].to(torch.float32), return_dict=False)\n                pred = unet(noisy_latents, timesteps, encoder_hidden_states=enc_hidden_states, down_block_additional_residuals=down, mid_block_additional_residual=mid).sample\n                \n                loss = torch.nn.functional.mse_loss(pred.float(), noise.float(), reduction=\"mean\")\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad()\n                \n            if accelerator.is_main_process: \n                pbar.update(1)\n                pbar.set_postfix(loss=loss.item())\n\n        if accelerator.is_main_process:\n            # --- THE FIX IS HERE ---\n            # We unwrap the model from the Multi-GPU container before saving\n            save_path = f\"{config.output_dir}/epoch_{epoch}\"\n            accelerator.unwrap_model(controlnet).save_pretrained(save_path)\n            print(f\"Saved epoch {epoch} to {save_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --multi_gpu --num_processes 2 --mixed_precision fp16 train_script.py","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **METRICS**","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw\nfrom tqdm.auto import tqdm\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\nfrom torchmetrics.image import StructuralSimilarityIndexMeasure\nfrom pycocotools.coco import COCO\nfrom torchvision import transforms\n\n# --- CONFIGURATION ---\n# The list of epochs you want to compare\nepochs_to_test = [\"epoch_9\"]\n\n# We define WHERE to look for each model.\n# The code will check these paths in order for each epoch name.\nsearch_paths = [\n    \"/kaggle/working/controlnet-layout-model\" # Check the newly trained one\n]\n\nnum_samples = 2  # 50 samples for a good score\n\n# --- SETUP METRICS ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚öôÔ∏è Setting up evaluation on {device}...\")\n\n# LPIPS (Perceptual Loss) - Lower is Better\nlpips_metric = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(device)\n# SSIM (Structural Similarity) - Higher is Better\nssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n\n# --- DATASET HELPER ---\ncoco_root = \"/kaggle/input/coco-2017-dataset/coco2017\"\nimg_dir = os.path.join(coco_root, \"train2017\")\nann_file = os.path.join(coco_root, \"annotations/instances_train2017.json\")\ncoco = COCO(ann_file)\n\ndef render_layout_mask(img_size, annotations):\n    canvas = Image.new(\"RGB\", img_size, (0, 0, 0))\n    draw = ImageDraw.Draw(canvas)\n    for ann in annotations:\n        x, y, w, h = ann['bbox']\n        draw.rectangle([x, y, x + w, y + h], outline=(255, 255, 255), width=2)\n    return canvas\n\ndef get_eval_batch(num_samples):\n    # Get random IDs that have annotations\n    ids = list(coco.imgs.keys())\n    ids = [i for i in ids if len(coco.getAnnIds(imgIds=i)) > 0]\n    random.seed(42) # Fixed seed ensures we test on the SAME images for every model\n    test_ids = random.sample(ids, num_samples)\n    \n    batch_data = []\n    for img_id in test_ids:\n        # Load Real Image (Ground Truth)\n        img_info = coco.loadImgs(img_id)[0]\n        img_path = os.path.join(img_dir, img_info['file_name'])\n        original_img = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n        \n        # Load Annotations & Make Mask\n        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n        scale_x = 512 / img_info['width']\n        scale_y = 512 / img_info['height']\n        scaled_anns = [{'bbox': [a['bbox'][0]*scale_x, a['bbox'][1]*scale_y, a['bbox'][2]*scale_x, a['bbox'][3]*scale_y]} for a in anns]\n        mask = render_layout_mask((512, 512), scaled_anns)\n        \n        # Make Prompt\n        cats = coco.loadCats([a['category_id'] for a in anns])\n        names = list(set([c['name'] for c in cats]))\n        prompt = f\"A photorealistic image comprising {', '.join(names)}\"\n        \n        batch_data.append({\"real\": original_img, \"mask\": mask, \"prompt\": prompt})\n    return batch_data\n\ndef find_model_path(epoch_name, search_paths):\n    \"\"\"\n    Intelligently searches for the folder 'epoch_X' inside the search paths.\n    \"\"\"\n    for base_path in search_paths:\n        # Check 1: Is it directly inside? (e.g. /kaggle/input/my-dataset/epoch_0)\n        candidate = os.path.join(base_path, epoch_name)\n        if os.path.exists(candidate):\n            return candidate\n            \n        # Check 2: Sometimes uploading zips creates a double folder structure\n        # (e.g. /kaggle/input/my-dataset/epoch_0/epoch_0)\n        candidate_nested = os.path.join(base_path, epoch_name, epoch_name)\n        if os.path.exists(candidate_nested):\n            return candidate_nested\n\n    return None\n\n# --- EVALUATION LOOP ---\nresults = []\ntest_data = get_eval_batch(num_samples)\nprint(f\"üì¶ Prepared {len(test_data)} samples for evaluation.\")\n\nto_tensor = transforms.ToTensor()\n\nfor epoch_name in epochs_to_test:\n    # 1. FIND THE MODEL\n    model_path = find_model_path(epoch_name, search_paths)\n    \n    if model_path is None:\n        print(f\"‚ö†Ô∏è SKIPPING {epoch_name}: Could not find folder in any search path.\")\n        continue\n\n    print(f\"\\nüöÄ Evaluating {epoch_name} from: {model_path}\")\n    \n    try:\n        # 2. LOAD MODEL\n        controlnet = ControlNetModel.from_pretrained(model_path, torch_dtype=torch.float16)\n        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None\n        ).to(\"cuda\")\n        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n        pipe.set_progress_bar_config(disable=True) # Silence individual progress bars\n        pipe.enable_model_cpu_offload() # Save VRAM\n        \n        lpips_scores = []\n        ssim_scores = []\n        \n        # 3. RUN INFERENCE\n        for item in tqdm(test_data, desc=f\"Testing {epoch_name}\"):\n            # Generate (Fixed seed 42 for consistency across models)\n            gen = torch.manual_seed(42)\n            \n            gen_img = pipe(\n                item[\"prompt\"], \n                image=item[\"mask\"], \n                num_inference_steps=20,\n                guidance_scale=7.5,\n                generator=gen\n            ).images[0]\n            \n            # Convert to Tensor for Metrics (normalize to 0-1)\n            real_t = to_tensor(item[\"real\"]).unsqueeze(0).to(device)\n            gen_t = to_tensor(gen_img).unsqueeze(0).to(device)\n            \n            # Compute Metrics\n            with torch.no_grad():\n                # LPIPS expects input in [0, 1] or [-1, 1], we provide [0, 1]\n                l_score = lpips_metric(gen_t, real_t)\n                s_score = ssim_metric(gen_t, real_t)\n            \n            lpips_scores.append(l_score.item())\n            ssim_scores.append(s_score.item())\n            \n        avg_lpips = np.mean(lpips_scores)\n        avg_ssim = np.mean(ssim_scores)\n        \n        print(f\"   üìä Results: LPIPS: {avg_lpips:.4f} | SSIM: {avg_ssim:.4f}\")\n        results.append({\n            \"Epoch\": epoch_name, \n            \"LPIPS (Lower is Better)\": avg_lpips, \n            \"SSIM (Higher is Better)\": avg_ssim\n        })\n        \n        # Cleanup memory for next model\n        del pipe, controlnet\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"   ‚ùå Error evaluating {epoch_name}: {e}\")\n\n# --- FINAL TABLE ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL QUANTITATIVE RESULTS\")\nprint(\"=\"*60)\ndf = pd.DataFrame(results)\nprint(df.to_string(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **VISUALISATION OF IMAGES**","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom pycocotools.coco import COCO\n\n# --- 1. CONFIGURATION ---\nNUM_IMAGES = 4  # Number of comparison rows\noutput_dir = \"/kaggle/working/controlnet-layout-model\"\nepoch_0_path = os.path.join(output_dir, \"epoch_0\")\nepoch_9_path = os.path.join(output_dir, \"epoch_9\")\n\nprint(f\"üìâ Epoch 0 Path: {epoch_0_path}\")\nprint(f\"üìà Epoch 9 Path: {epoch_9_path}\")\n\n# --- 2. DATASET & FILTERING ---\ncoco_root = \"/kaggle/input/coco-2017-dataset/coco2017\"\nimg_dir = os.path.join(coco_root, \"train2017\")\nann_file = os.path.join(coco_root, \"annotations/instances_train2017.json\")\ncoco = COCO(ann_file)\n\ndef find_simple_images(coco, limit=5):\n    target_cats = ['dog', 'cat', 'horse', 'bird', 'airplane', 'bus']\n    cat_ids = coco.getCatIds(catNms=target_cats)\n    found_ids = []\n    \n    random.shuffle(cat_ids)\n    for cat_id in cat_ids:\n        img_ids = coco.getImgIds(catIds=[cat_id])\n        random.shuffle(img_ids)\n        for img_id in img_ids:\n            if img_id in found_ids: continue\n            anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n            # Strict filter: 1-2 objects, large area\n            if 1 <= len(anns) <= 2:\n                if all(ann['area'] > 40000 for ann in anns):\n                    found_ids.append(img_id)\n                    break \n        if len(found_ids) >= limit: break\n    return found_ids\n\ndef render_layout_mask(img_size, annotations):\n    canvas = Image.new(\"RGB\", img_size, (0, 0, 0))\n    draw = ImageDraw.Draw(canvas)\n    for ann in annotations:\n        x, y, w, h = ann['bbox']\n        draw.rectangle([x, y, x + w, y + h], outline=(255, 255, 255), width=2)\n    return canvas\n\n# --- 3. PREPARE DATA (Images & Prompts) ---\ncurated_ids = find_simple_images(coco, limit=NUM_IMAGES)\ndata_batch = []\n\nprint(f\"üì¶ Preparing {len(curated_ids)} test cases...\")\n\nfor img_id in curated_ids:\n    # Load Real\n    img_info = coco.loadImgs(img_id)[0]\n    img_path = os.path.join(img_dir, img_info['file_name'])\n    original_img = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n    \n    # Load Anns\n    anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n    scale_x, scale_y = 512 / img_info['width'], 512 / img_info['height']\n    scaled_anns = [{'bbox': [a['bbox'][0]*scale_x, a['bbox'][1]*scale_y, a['bbox'][2]*scale_x, a['bbox'][3]*scale_y]} for a in anns]\n    \n    # Create Mask\n    control_mask = render_layout_mask((512, 512), scaled_anns)\n    \n    # Create Prompt\n    cats = coco.loadCats([a['category_id'] for a in anns])\n    names = list(set([c['name'] for c in cats]))\n    prompt = f\"A photorealistic image comprising {', '.join(names)}\"\n    \n    data_batch.append({\n        \"id\": img_id,\n        \"real\": original_img,\n        \"mask\": control_mask,\n        \"prompt\": prompt,\n        \"epoch_0_img\": None,\n        \"epoch_9_img\": None\n    })\n\n# --- 4. GENERATION LOOP (Model Swapping) ---\n\n# A. Generate with EPOCH 0\nprint(\"\\nü§ñ Loading EPOCH 0 Model...\")\ncnet_0 = ControlNetModel.from_pretrained(epoch_0_path, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=cnet_0, torch_dtype=torch.float16, safety_checker=None\n).to(\"cuda\")\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprint(\"   Generating Epoch 0 outputs...\")\nfor i, item in enumerate(data_batch):\n    gen = torch.manual_seed(100 + i)\n    item[\"epoch_0_img\"] = pipe(item[\"prompt\"], image=item[\"mask\"], num_inference_steps=20, generator=gen).images[0]\n\n# Cleanup Memory\ndel cnet_0, pipe\ntorch.cuda.empty_cache()\n\n# B. Generate with EPOCH 9\nprint(\"\\nüöÄ Loading EPOCH 9 Model...\")\ncnet_9 = ControlNetModel.from_pretrained(epoch_9_path, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=cnet_9, torch_dtype=torch.float16, safety_checker=None\n).to(\"cuda\")\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprint(\"   Generating Epoch 9 outputs...\")\nfor i, item in enumerate(data_batch):\n    gen = torch.manual_seed(100 + i) # SAME SEED ensures fair comparison\n    item[\"epoch_9_img\"] = pipe(item[\"prompt\"], image=item[\"mask\"], num_inference_steps=20, generator=gen).images[0]\n\n# --- 5. FINAL VISUALIZATION ---\nprint(\"\\nüé® Displaying Results...\")\nfor item in data_batch:\n    fig, axs = plt.subplots(1, 4, figsize=(20, 5)) \n    \n    # Column 1: Layout\n    axs[0].imshow(item[\"mask\"])\n    axs[0].set_title(\"Input Layout (Control)\")\n    axs[0].axis(\"off\")\n    \n    # Column 2: Ground Truth\n    axs[1].imshow(item[\"real\"])\n    axs[1].set_title(\"Ground Truth (Real)\")\n    axs[1].axis(\"off\")\n    \n    # Column 3: Epoch 0\n    axs[2].imshow(item[\"epoch_9_img\"])\n    axs[2].set_title(\"Epoch 0 (Untrained)\")\n    axs[2].axis(\"off\")\n    \n    # Column 4: Epoch 9\n    axs[3].imshow(item[\"epoch_9_img\"])\n    axs[3].set_title(\"Epoch 9 (Final)\")\n    axs[3].axis(\"off\")\n    \n    plt.show()","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null}]}