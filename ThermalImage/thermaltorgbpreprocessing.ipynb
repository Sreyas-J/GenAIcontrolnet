{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1428091,"sourceType":"datasetVersion","datasetId":836348}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:36:54.860537Z","iopub.execute_input":"2025-12-10T14:36:54.860798Z","iopub.status.idle":"2025-12-10T14:36:57.152486Z","shell.execute_reply.started":"2025-12-10T14:36:54.860778Z","shell.execute_reply":"2025-12-10T14:36:57.151700Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 1: Install Dependencies\n\n### We need transformers for the auto-captioning (BLIP) and diffusers for later training.","metadata":{}},{"cell_type":"code","source":"# Install dependencies for Preprocessing only (No xformers needed yet)\n!pip install -q diffusers transformers accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:36:59.666088Z","iopub.execute_input":"2025-12-10T14:36:59.666963Z","iopub.status.idle":"2025-12-10T14:38:20.956198Z","shell.execute_reply.started":"2025-12-10T14:36:59.666927Z","shell.execute_reply":"2025-12-10T14:38:20.955434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    import diffusers\n    import transformers\n    import torch\n    print(\"SUCCESS: All necessary libraries are imported correctly!\")\n    print(f\"Torch version: {torch.__version__}\")\nexcept ImportError as e:\n    print(f\"FAILED: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:39:09.820598Z","iopub.execute_input":"2025-12-10T14:39:09.821542Z","iopub.status.idle":"2025-12-10T14:39:23.967886Z","shell.execute_reply.started":"2025-12-10T14:39:09.821505Z","shell.execute_reply":"2025-12-10T14:39:23.967097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 3: The Preprocessing Script\n\n### This script fulfills the requirement to \"Construct a dataset with paired images, conditions, and prompts\". What this code does:\n\n    Auto-Captions: Uses the GPU to generate captions via BLIP.\n\n    Formats: Saves everything to /kaggle/working/, which is the writeable area in Kaggle.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport json\nimport torch\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# ================= CONFIGURATION =================\n# Root paths (Verify these match your Kaggle input structure)\nBASE_PATH = \"/kaggle/input/flir-thermal-images-dataset/FLIR_ADAS_1_3\"\n\n# Define the tasks: (Input RGB, Input Thermal, Output Split Name)\nTASKS = [\n    {\n        \"rgb_dir\": f\"{BASE_PATH}/train/RGB\",\n        \"thermal_dir\": f\"{BASE_PATH}/train/thermal_8_bit\",\n        \"split_name\": \"train\",\n        \"limit\": 2000  # Limit training images to save time/space? (Set None for all)\n    },\n    {\n        \"rgb_dir\": f\"{BASE_PATH}/val/RGB\",\n        \"thermal_dir\": f\"{BASE_PATH}/val/thermal_8_bit\",\n        \"split_name\": \"validation\",\n        \"limit\": None  # Usually keep all validation images (dataset is smaller)\n    }\n]\n\n# Output Location\nWORKING_DIR = \"/kaggle/working/controlnet_dataset\"\nRESOLUTION = 512\n# =================================================\n\ndef setup_caption_model():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Loading BLIP captioning model on {device}...\")\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n    return processor, model, device\n\ndef process_split(task, processor, model, device):\n    \"\"\"Processes a single split (train or validation)\"\"\"\n    split = task['split_name']\n    print(f\"\\n--- Processing Split: {split.upper()} ---\")\n    \n    # 1. Create Split Directories\n    split_dir = os.path.join(WORKING_DIR, split)\n    os.makedirs(f\"{split_dir}/images\", exist_ok=True)\n    os.makedirs(f\"{split_dir}/conditioning_images\", exist_ok=True)\n\n    # 2. Get Files\n    if not os.path.exists(task['rgb_dir']):\n        print(f\"WARNING: Path not found {task['rgb_dir']}\")\n        return\n\n    all_files = sorted([f for f in os.listdir(task['rgb_dir']) if f.endswith('.jpg')])\n    \n    # Apply limit if set\n    if task['limit']:\n        all_files = all_files[:task['limit']]\n        \n    print(f\"Found {len(all_files)} images. Starting processing...\")\n    metadata = []\n\n    for filename in tqdm(all_files):\n        rgb_path = os.path.join(task['rgb_dir'], filename)\n        \n        # Handle Thermal Extension Mismatch (.jpg vs .jpeg)\n        thermal_name = filename.replace(\".jpg\", \".jpeg\")\n        thermal_path = os.path.join(task['thermal_dir'], thermal_name)\n        \n        # Fallback to original name if .jpeg doesn't exist\n        if not os.path.exists(thermal_path):\n             thermal_path = os.path.join(task['thermal_dir'], filename)\n\n        if os.path.exists(thermal_path):\n            try:\n                # Resize\n                img_rgb = Image.open(rgb_path).convert(\"RGB\").resize((RESOLUTION, RESOLUTION))\n                img_thermal = Image.open(thermal_path).convert(\"L\").resize((RESOLUTION, RESOLUTION))\n                \n                # Caption (Only need to generate once, used for both)\n                inputs = processor(img_rgb, return_tensors=\"pt\").to(device)\n                out = model.generate(**inputs, max_new_tokens=50)\n                caption = processor.decode(out[0], skip_special_tokens=True)\n                \n                # Save Images (PNG is safer for lossless)\n                save_name = filename.replace(\".jpg\", \".png\").replace(\".jpeg\", \".png\")\n                \n                # Note: We save RELATIVE paths in the metadata for HuggingFace ImageFolder\n                img_rgb.save(f\"{split_dir}/images/{save_name}\")\n                img_thermal.save(f\"{split_dir}/conditioning_images/{save_name}\")\n                \n                metadata.append({\n                    \"file_name\": f\"images/{save_name}\",\n                    \"conditioning_image\": f\"conditioning_images/{save_name}\",\n                    \"text\": caption\n                })\n            except Exception as e:\n                print(f\"Error processing {filename}: {e}\")\n\n    # 3. Save JSONL Metadata\n    # Hugging Face ImageFolder expects 'metadata.jsonl' in the split folder\n    with open(f\"{split_dir}/metadata.jsonl\", 'w') as f:\n        for line in metadata:\n            f.write(json.dumps(line) + \"\\n\")\n            \n    print(f\"Completed {split}. Saved {len(metadata)} pairs.\")\n\ndef main():\n    # Setup\n    if os.path.exists(WORKING_DIR): shutil.rmtree(WORKING_DIR)\n    processor, model, device = setup_caption_model()\n    \n    # Process both Train and Val\n    for task in TASKS:\n        process_split(task, processor, model, device)\n        \n    # Zip Everything\n    print(\"\\nZipping dataset...\")\n    shutil.make_archive(\"/kaggle/working/flir_controlnet_split\", 'zip', WORKING_DIR)\n    print(\"DONE! Download 'flir_controlnet_split.zip' from Output.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:40:12.929492Z","iopub.execute_input":"2025-12-10T14:40:12.930186Z","iopub.status.idle":"2025-12-10T15:03:12.001079Z","shell.execute_reply.started":"2025-12-10T14:40:12.930163Z","shell.execute_reply":"2025-12-10T15:03:12.000186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 4: Visualize Your Data (Quality Check)\n\n### The guidelines require you to show \"Condition â†’ Output grids\". Use this code to verify your data looks correct before training.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\nimport json\nimport os\nfrom PIL import Image\n\n# ================= CONFIGURATION =================\n# Set this to the same directory used in the processing script\nBASE_DIR = \"/kaggle/working/controlnet_dataset\" \nSPLIT = \"validation\"  # Change to 'validation' to check the other split\n# =================================================\n\n# 1. Define paths based on the split\nsplit_dir = os.path.join(BASE_DIR, SPLIT)\nmetadata_path = os.path.join(split_dir, \"metadata.jsonl\")\n\n# 2. Load the metadata\nif not os.path.exists(metadata_path):\n    print(f\"Error: Metadata file not found at {metadata_path}\")\nelse:\n    with open(metadata_path, 'r') as f:\n        lines = f.readlines()\n\n    if not lines:\n        print(\"Error: Metadata file is empty.\")\n    else:\n        # 3. Pick a random sample\n        sample = json.loads(random.choice(lines))\n\n        # 4. Construct full image paths\n        # Note: metadata stores paths like \"images/file.png\", so we join with split_dir\n        target_path = os.path.join(split_dir, sample['file_name'])\n        cond_path = os.path.join(split_dir, sample['conditioning_image'])\n\n        # 5. Load Images\n        try:\n            img_target = Image.open(target_path)\n            img_cond = Image.open(cond_path)\n\n            # 6. Plot\n            fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n            \n            # Show Thermal Condition\n            ax[0].imshow(img_cond, cmap='gray')\n            ax[0].set_title(\"Condition (Thermal Input)\")\n            ax[0].axis('off')\n\n            # Show RGB Target with Caption\n            ax[1].imshow(img_target)\n            # Wrap text if it's too long\n            caption = sample['text']\n            title_text = f\"Target (RGB)\\nPrompt: {caption[:50]}...\" if len(caption) > 50 else f\"Target (RGB)\\nPrompt: {caption}\"\n            \n            ax[1].set_title(title_text, fontsize=10)\n            ax[1].axis('off')\n\n            plt.tight_layout()\n            plt.show()\n            \n            print(f\"Sampled from: {SPLIT}\")\n            print(f\"Full Caption: {caption}\")\n\n        except FileNotFoundError:\n            print(f\"Error: Could not find image files.\\nTarget: {target_path}\\nCond: {cond_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:03:37.623535Z","iopub.execute_input":"2025-12-10T15:03:37.624513Z","iopub.status.idle":"2025-12-10T15:03:38.100960Z","shell.execute_reply.started":"2025-12-10T15:03:37.624489Z","shell.execute_reply":"2025-12-10T15:03:38.100093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}